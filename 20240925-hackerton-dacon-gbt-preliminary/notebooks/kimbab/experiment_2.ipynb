{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71defaba-d5bc-4137-a15a-06a89ea65bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/Users/baeyeongmin/Desktop/workspace/dacon-2024-gbt-hackerton\")\n",
    "sys.path.append(\"/root/dacon-2024-gbt-hackerton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20f6030c-037c-4528-9289-9abab1bba0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mryanbae\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/dacon-2024-gbt-hackerton/notebooks/wandb/run-20241002_052224-4q9yglnl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ryanbae/dacon-gbt-2024-hackerton/runs/4q9yglnl' target=\"_blank\">valiant-rain-23</a></strong> to <a href='https://wandb.ai/ryanbae/dacon-gbt-2024-hackerton' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ryanbae/dacon-gbt-2024-hackerton' target=\"_blank\">https://wandb.ai/ryanbae/dacon-gbt-2024-hackerton</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ryanbae/dacon-gbt-2024-hackerton/runs/4q9yglnl' target=\"_blank\">https://wandb.ai/ryanbae/dacon-gbt-2024-hackerton/runs/4q9yglnl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import pandas as pd\n",
    "from src.config import CFG\n",
    "from src.data_processing import load_data, preprocess_data, get_label_encoded, split_data\n",
    "from src.models import get_model_and_tokenizer\n",
    "from src.datasets import TextDataset\n",
    "from src.training import train_model\n",
    "from src.inference import get_test_predictions\n",
    "from src import root_path\n",
    "from src.utils import get_sample_submission\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "CFG.model_type = \"primary\"\n",
    "wrun = wandb.init(project=\"dacon-gbt-2024-hackerton\", config=CFG)\n",
    "\n",
    "# Data loading and preprocessing\n",
    "train_df, test_df = load_data(root_path=root_path)\n",
    "train_df = preprocess_data(train_df)\n",
    "test_df = preprocess_data(test_df)\n",
    "\n",
    "train_df, label_encoder = get_label_encoded(train_df, model_type=CFG.model_type)\n",
    "train_df, val_df = split_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da9c1b5a-698e-4073-a6a7-40e092483aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be32e0a64a741458fb37865860eec31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39ff411262048a0b8a7c36a06271135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/241k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5201c5e58d904190ba88b02ff2f36a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/492k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18070080e36b4e27bd3526f88723d13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a42753885ca4714a36c1d84102b3ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/870 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba463f74d7654152bd76b19dac92f60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/458M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at monologg/kobigbird-bert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model and tokenizer initialization\n",
    "model, tokenizer = get_model_and_tokenizer(CFG.model_name, len(label_encoder), CFG.attention_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582c1256-4de2-47ef-82ad-83288d53c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 683/683 [05:24<00:00,  2.10it/s]\n",
      "Validating: 100%|██████████| 171/171 [00:35<00:00,  4.86it/s]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mnum_workers, pin_memory\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mpin_memory)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, CFG)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m     14\u001b[0m label_decoder \u001b[38;5;241m=\u001b[39m {i: label \u001b[38;5;28;01mfor\u001b[39;00m label, i \u001b[38;5;129;01min\u001b[39;00m label_encoder\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/dacon-2024-gbt-hackerton/src/training.py:47\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, config)\u001b[0m\n\u001b[1;32m     44\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     45\u001b[0m val_f1 \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, device)\n\u001b[0;32m---> 47\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_train_loss,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_f1,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_val_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_val_f1\n\u001b[1;32m     53\u001b[0m })\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader creation\n",
    "train_dataset = TextDataset(train_df.text.tolist(), train_df.label.tolist(), tokenizer, max_len=CFG.tokenizer_max_len)\n",
    "val_dataset = TextDataset(val_df.text.to_list(), val_df.label.tolist(), tokenizer, max_len=CFG.tokenizer_max_len)\n",
    "test_dataset = TextDataset(test_df.text.tolist(), None, tokenizer, max_len=CFG.tokenizer_max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=CFG.pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=CFG.pin_memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers, pin_memory=CFG.pin_memory)\n",
    "\n",
    "# Model training\n",
    "trained_model = train_model(model, train_loader, val_loader, CFG)\n",
    "\n",
    "# Inference\n",
    "label_decoder = {i: label for label, i in label_encoder.items()}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "predictions = get_test_predictions(trained_model, test_loader, device, label_decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041d8de-fc2e-4f00-8bf3-fa3f53e017a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
